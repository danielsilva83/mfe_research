{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pymfe\n",
    "import glob\n",
    "from pymfe.mfe import MFE\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\daniel.silva\\\\Documents\\\\mfes'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "cwd_back = os.path.dirname(cwd)\n",
    "data_path = os.path.join(cwd_back, 'mfes')\n",
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carrega ps datasets, extrai as meta caracterkstkcas e salva a em csv no save_dir path \n",
    "\n",
    "data_list = os.path.join(data_path, 'datasets') #data = Pasta local onde se encontra os dataset, arquivos .csv \n",
    "print(data_list)\n",
    "save_dir = 'C:\\\\Users\\\\daniel.silva\\\\Documents\\\\mfes\\\\mfe_extracted' \n",
    "\n",
    "all_files = glob.glob(data_list + \"\\*.csv\") #fazendo o carregamento dos datasets da pasta 'data' com os arquivos .csv\n",
    "\n",
    "\n",
    "mfe = MFE( groups=[\"general\", \"statistical\", \"info-theory\"], summary=[\"mean\", \"sd\", \"min\", \"median\", \"max\"]) # MFE Groups, passando 'ALL' com parametro para todos \n",
    "mfe_sumary = MFE(summary=[\"min\", \"median\", \"max\"]) # MFE Summary, passando parametros de sumarização 'minimo', 'media' e 'maximo'\n",
    "mfe_quantiles = MFE(summary=[\"quantiles\"])  # MFE Summary, passando parametros de sumarização \"quantiles\"\n",
    "\n",
    "files = [] # vetor para armazenar os Dataframes com os dados de cada dataset carregados\n",
    "i = 0 # variavel contadora para contabilizar o numero de repetições do experimento\n",
    "for file_ in all_files: #laço de repetição para ler todos os arquivos da pasta local 'data'\n",
    "    filepath_ = os.path.join(data_path, 'data', file_) # extraindo e armazenando o caminho e nome do arquivo .csv\n",
    "    filesx = pd.read_csv(file_,encoding = \"ISO-8859-1\", decimal=\",\", header=0)\n",
    "    filesx = filesx.fillna(file_)\n",
    "    files.append(filesx) \n",
    "   \n",
    "nfiles =  0   #variavel contadora para armazenar o numero de arquivos carregados\n",
    "for df in files[0:103]: #laço para iterara o vetor com os datafremes que foram armazenados\n",
    "   \n",
    "    ########################## inicio processamento dos datasets originais no MFE ######################################\n",
    "        print(\"\\nArquivo numero: \",nfiles) #printando o numero do arquivo \n",
    "       \n",
    "        list_df_origin = df.values.tolist() # transformando o dataframe em uma lista para passar para o MFE\n",
    "        \n",
    "        mfe.fit(list_df_origin) # passando a lista de dados para o MFE \n",
    "        try:\n",
    "                ft = mfe.extract() # extraindo as metricas do MFE Group (All)\n",
    "\n",
    "        except pymfe.mfe.MFEError:\n",
    "                print(\"Erro no arquivo: \")\n",
    "                continue\n",
    "                \n",
    "        df_ft_ = pd.DataFrame (ft, columns=['attr_conc.max', 'attr_conc.mean', 'attr_conc.median', 'attr_conc.min', 'attr_conc.sd', 'attr_ent.max', 'attr_ent.mean', 'attr_ent.median', 'attr_ent.min', 'attr_ent.sd', 'attr_to_inst', 'cat_to_num', 'cor.max', 'cor.mean', 'cor.median', 'cor.min', 'cor.sd', 'cov.max', 'cov.mean', 'cov.median', 'cov.min', 'cov.sd', 'eigenvalues.max', 'eigenvalues.mean', 'eigenvalues.median', 'eigenvalues.min', 'eigenvalues.sd', 'g_mean.max', 'g_mean.mean', 'g_mean.median', 'g_mean.min', 'g_mean.sd', 'h_mean.max', 'h_mean.mean', 'h_mean.median', 'h_mean.min', 'h_mean.sd', 'inst_to_attr', 'iq_range.max', 'iq_range.mean', 'iq_range.median', 'iq_range.min', 'iq_range.sd', 'kurtosis.max', 'kurtosis.mean', 'kurtosis.median', 'kurtosis.min', 'kurtosis.sd', 'mad.max', 'mad.mean', 'mad.median', 'mad.min', 'mad.sd', 'max.max', 'max.mean', 'max.median', 'max.min', 'max.sd', 'mean.max', 'mean.mean', 'mean.median', 'mean.min', 'mean.sd', 'median.max', 'median.mean', 'median.median', 'median.min', 'median.sd', 'min.max', 'min.mean', 'min.median', 'min.min', 'min.sd', 'nr_attr', 'nr_bin', 'nr_cat', 'nr_cor_attr', 'nr_inst', 'nr_norm', 'nr_num', 'nr_outliers', 'num_to_cat', 'range.max', 'range.mean', 'range.median', 'range.min', 'range.sd', 'sd.max', 'sd.mean', 'sd.median', 'sd.min', 'sd.sd', 'skewness.max', 'skewness.mean', 'skewness.median', 'skewness.min', 'skewness.sd', 'sparsity.max', 'sparsity.mean', 'sparsity.median', 'sparsity.min', 'sparsity.sd', 't_mean.max', 't_mean.mean', 't_mean.median', 't_mean.min', 't_mean.sd', 'var.max', 'var.mean', 'var.median', 'var.min', 'var.sd'])  #transformando a saida do MFE em um dataframe)\n",
    "     \n",
    "        df_ft = pd.DataFrame (ft) #transformando a saida do MFE em um dataframe\n",
    "    \n",
    "        df_ft.to_csv(os.path.join(save_dir, f\"{nfiles}_original.csv\"), index=False)\n",
    "\n",
    "        nfiles = nfiles+1\n",
    "        \n",
    "       \n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = os.path.join(data_path, 'mfe_extracted') #data = Pasta local onde se encontra os dataset, arquivos .csv \n",
    "print(data_list)\n",
    "save_dir = 'C:\\\\Users\\\\daniel.silva\\\\Documents\\\\mfes' \n",
    "\n",
    "all_files = glob.glob(data_list + \"\\*.csv\") #fazendo o carregamento dos datasets da pasta 'data' com os arquivos .csv\n",
    "\n",
    "# Lista para armazenar os DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Ler cada arquivo CSV e armazenar o DataFrame na lista dataframes\n",
    "for nome_arquivo in all_files:\n",
    "    df = pd.read_csv(nome_arquivo)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatenar todos os DataFrames em um único DataFrame\n",
    "df_final = pd.concat(dataframes, ignore_index=True)\n",
    "df_final.to_csv(os.path.join(save_dir, \"arquivo_final.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path + \"\\\\arquivo_final.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Criar um dicionário para armazenar os resultados\n",
    "resultados_dict = {'Coluna alvo': [], 'R²': [], 'MAE': [], 'MSE': [], 'RMSE': [], 'MAPE': [], 'MPE': []}\n",
    "\n",
    "# Iterar sobre todas as colunas\n",
    "for target_column in df.columns:\n",
    "    # Preencher os valores ausentes na coluna alvo\n",
    "    df[target_column].fillna(df[target_column].mean(), inplace=True)\n",
    "    \n",
    "    # Dividir os dados em features (atributos) e o target\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "    \n",
    "    # Dividir os dados em conjuntos de treinamento e teste\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Instanciar o modelo RandomForestRegressor\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    \n",
    "    # Treinar o modelo usando o conjunto de treinamento\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Fazer previsões\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calcular as métricas de avaliação\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "    mpe = np.mean((y_test - y_pred) / y_test) * 100\n",
    "    \n",
    "    resultados_dict['Coluna alvo'].append(target_column)\n",
    "    resultados_dict['R²'].append(r2)\n",
    "    resultados_dict['MAE'].append(mae)\n",
    "    resultados_dict['MSE'].append(mse)\n",
    "    resultados_dict['RMSE'].append(rmse)\n",
    "    resultados_dict['MAPE'].append(mape)\n",
    "    resultados_dict['MPE'].append(mpe)\n",
    "\n",
    "df_final = pd.DataFrame(resultados_dict)\n",
    "\n",
    "df_final.to_csv('resultados_modelos.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
